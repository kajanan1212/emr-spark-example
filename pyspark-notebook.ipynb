{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b4d20ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, countDistinct, to_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8eea13",
   "metadata": {},
   "source": [
    "### Task 2: Importing and Managing Data in Amazon S3 (Marks: 5/25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70d1b21",
   "metadata": {},
   "source": [
    "#### 2.2) List the files present in your Amazon S3 bucket. Provide screenshots of the command execution and the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a193b5aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1c5fc08",
   "metadata": {},
   "source": [
    "### Task 3: Data Processing with Apache Spark via Amazon EMR (Marks: 15/25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc13ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('a01-big-data-analytics').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ed007b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eca0aa9",
   "metadata": {},
   "source": [
    "#### 3.1 Spark DataFrame Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc825f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.read.csv('./data/voice_sample.csv', header=True, inferSchema=True)\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d294629",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63aeab7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a70f512",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.withColumn('CALL_DATE', to_date('CALL_TIME', 'yyyyMMddHHmmss'))\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db79bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f179a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.select('CALL_DATE').distinct().sort('CALL_DATE').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeb6144",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_NO_OF_DISTINCT_CALL_DATE = df1.select('CALL_DATE').distinct().count()\n",
    "TOTAL_NO_OF_DISTINCT_CALL_DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c411fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.agg(countDistinct('CALL_DATE')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2ab4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read.csv('./data/cell_centers.csv', header=True, inferSchema=True)\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4b28c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d8cd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca78688",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.select('PROVINCE_NAME').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98157fd9",
   "metadata": {},
   "source": [
    "##### 3.1.1) Using an Amazon EMR notebook and Spark DataFrame API, extract the unique CALLER_IDs of users who have made at least one call every day. The calls must have been made from the Western Province. Include the Spark commands used and the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe086a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = (\n",
    "    df2\n",
    "    .filter(df2['PROVINCE_NAME'] == 'Western')\n",
    "    .join(df1, ['LOCATION_ID'])\n",
    "    .groupBy('CALLER_ID').agg(countDistinct('CALL_DATE').alias('NO_OF_DISTINCT_CALL_DATE'))\n",
    "    .filter(col('NO_OF_DISTINCT_CALL_DATE') == df1.select('CALL_DATE').distinct().count())\n",
    "    .select('CALLER_ID')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca0d343",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65817d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d239832",
   "metadata": {},
   "source": [
    "#### 3.2 Spark SQL Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aeb2762",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.read.csv('./data/voice_sample.csv', header=True, inferSchema=True)\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09890709",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read.csv('./data/cell_centers.csv', header=True, inferSchema=True)\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9154bd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.createOrReplaceTempView(\"voice_sample\")\n",
    "df2.createOrReplaceTempView(\"cell_centers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b8e4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.sql('SELECT *, TO_DATE(CALL_TIME, \"yyyyMMddHHmmss\") AS CALL_DATE FROM voice_sample')\n",
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c00db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.createOrReplaceTempView(\"voice_sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06699251",
   "metadata": {},
   "source": [
    "##### 3.2.1) Repeat the same task as in 3.1.1 but use Spark SQL for data extraction. Include the Spark SQL commands used and the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a07e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = spark.sql(\n",
    "    '''\n",
    "    SELECT CALLER_ID\n",
    "    FROM (\n",
    "        SELECT CALLER_ID, COUNT(DISTINCT CALL_DATE) AS NO_OF_DISTINCT_CALL_DATE\n",
    "        FROM (\n",
    "            SELECT LOCATION_ID\n",
    "            FROM cell_centers\n",
    "            WHERE PROVINCE_NAME = 'Western'\n",
    "        )\n",
    "        LEFT JOIN voice_sample\n",
    "        USING (LOCATION_ID)\n",
    "        GROUP BY CALLER_ID\n",
    "    )\n",
    "    WHERE NO_OF_DISTINCT_CALL_DATE = (SELECT COUNT(DISTINCT CALL_DATE) FROM voice_sample)\n",
    "    '''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30caa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25f9f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a440cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
